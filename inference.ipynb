{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54d16f83-eccd-44d1-8a93-ded3b19ae1d6",
      "metadata": {},
      "source": [
        "# Minimal Cataract Video Q&A — **Fine‑tuned (Headless‑Safe)**\n",
        "Loads your fine‑tuned Qwen2.5‑VL if present (local adapters/full model or Hub)."
      ]
    },
    {
      "cell_type": "code",
      "id": "b6384f22-021e-4076-8522-d5a24b3b576a",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n# Headless-friendly setup: disable ipywidgets/tqdm progress bars\nimport os\nos.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\nos.environ[\"TQDM_DISABLE\"] = \"1\"\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntry:\n    from huggingface_hub.utils import disable_progress_bars\n    disable_progress_bars()\nexcept Exception:\n    pass\nprint(\"Progress bars disabled for headless execution.\")\n"
    },
    {
      "cell_type": "markdown",
      "id": "b4283088-c50e-4f5d-84d5-17922e4e7c29",
      "metadata": {},
      "source": [
        "## ✅ Extra — Set paths / IDs\n",
        "- `FINETUNED_DIR`: local output of `train.py --save_dir` *(for adapters use `--save_adapter` during training)*\n",
        "- `HUB_MODEL_ID`: e.g. `yourname/qwen2.5-vl-7b-instruct-cataract1k` if you pushed to hub\n",
        "- `BASE_MODEL_ID`: base model id (defaults to 7B Instruct)"
      ]
    },
    {
      "cell_type": "code",
      "id": "7cbb9515-b0c7-4397-9759-990209a3f52d",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\nimport os, pathlib\nfrom IPython.display import Video\n\nhome = os.getenv(\"HOME\", \"~\")\nvideo_path = os.environ.get(\"VIDEO_PATH\", f\"{home}/surgery-sft/datasets/cataract1k/videos/test/case_4687_Capsulorhexis_52.50_57.50.mp4\")\n\n# Where your fine-tuned assets live (local)\nfinetuned_dir = os.environ.get(\"FINETUNED_DIR\", \"./qwen2.5-vl-7b-instruct-cataract1k\")\n\n# If you pushed to the Hub during training\nhub_model_id = os.environ.get(\"HUB_MODEL_ID\", \"\")\n\n# Base model to start from (if adapters are used)\nbase_model_id = os.environ.get(\"BASE_MODEL_ID\", \"Qwen/Qwen2.5-VL-7B-Instruct\")\n\nprint(\"Video:         \", video_path)\nprint(\"FINETUNED_DIR: \", finetuned_dir)\nprint(\"HUB_MODEL_ID:  \", hub_model_id or \"(none)\")\nprint(\"BASE_MODEL_ID: \", base_model_id)\n\nassert pathlib.Path(video_path).expanduser().exists(), f\"Video not found: {video_path}\"\n\n# Show the video inline (harmless under nbconvert)\nVideo(video_path, embed=True)\n"
    },
    {
      "cell_type": "markdown",
      "id": "d8cdfcf2-8931-4ecf-9d91-51da7e29a403",
      "metadata": {},
      "source": [
        "## ✅ Extra — Type your question"
      ]
    },
    {
      "cell_type": "code",
      "id": "cb6e4e9d-cd94-4492-92e5-2e6d91427479",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "question = \"Which phase of the surgery are we currently at?\"; print('Question:', question)"
    },
    {
      "cell_type": "code",
      "id": "3f8d05e8-bbb8-400a-8cf0-6222e112b2f4",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n# Load base + fine-tuned (adapters/full/Hub) with low VRAM; fall back requires user to set BASE_MODEL_ID\nimport os, json, torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor, BitsAndBytesConfig\nfrom peft import PeftModel\n\nuse_cuda = torch.cuda.is_available()\ndtype = torch.bfloat16 if use_cuda else torch.float32\n\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_type=torch.bfloat16,\n) if use_cuda else None\n\nprint(\"Loading base:\", base_model_id)\nbase = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    base_model_id,\n    device_map=\"auto\" if use_cuda else None,\n    dtype=dtype,\n    quantization_config=bnb_cfg,\n)\n\ndef try_load_peft(model_base, model_id_or_dir):\n    try:\n        m = PeftModel.from_pretrained(model_base, model_id_or_dir)\n        return m.merge_and_unload()\n    except Exception as e:\n        print(f\"PEFT load failed for '{model_id_or_dir}':\", e)\n        return None\n\ndef try_load_full(model_id_or_dir):\n    try:\n        return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            model_id_or_dir,\n            device_map=\"auto\" if use_cuda else None,\n            dtype=dtype,\n            quantization_config=bnb_cfg,\n        )\n    except Exception as e:\n        print(f\"Direct full-model load failed for '{model_id_or_dir}':\", e)\n        return None\n\nmodel = None\nload_from = None\n\n# 1) Try local adapters\nif os.path.isdir(finetuned_dir):\n    model = try_load_peft(base, finetuned_dir)\n    if model:\n        load_from = f\"local adapters: {finetuned_dir}\"\n    else:\n        # 2) Try local full model\n        model = try_load_full(finetuned_dir)\n        if model:\n            load_from = f\"local full model: {finetuned_dir}\"\n\n# 3) Try Hub if provided\nif model is None and hub_model_id:\n    model = try_load_peft(base, hub_model_id) or try_load_full(hub_model_id)\n    if model:\n        load_from = f\"hub: {hub_model_id}\"\n\nif model is None:\n    raise RuntimeError(\n        \"Could not find fine‑tuned weights.\\n\"\n        f\"Checked FINETUNED_DIR='{finetuned_dir}' and HUB_MODEL_ID='{hub_model_id}'.\\n\"\n        \"If you trained with QLoRA, ensure --save_adapter was used and FINETUNED_DIR contains adapter_config.json.\\n\"\n        \"Otherwise, set HUB_MODEL_ID to your pushed repo id.\\n\"\n    )\n\nprocessor = Qwen2_5_VLProcessor.from_pretrained(base_model_id, padding_side=\\\"right\\\", use_fast=True)\nmodel.eval()\nprint(\\\"✅ Loaded fine‑tuned from:\\\", load_from)\nprint(\\\"CUDA:\\\", use_cuda)\n"
    },
    {
      "cell_type": "code",
      "id": "2e7aa71a-39a4-431f-a50b-1096bbeb820e",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n# Sample a few frames safely on CPU\nimport decord, torch\ndecord.bridge.set_bridge('torch')\n\nvr = decord.VideoReader(video_path, ctx=decord.cpu(0))\nn = vr._num_frame\nfps = max(1, int(vr.get_avg_fps()) or 1)\n\ntarget = min(4, max(1, n // max(1, fps)))  # <=4 frames\nstep = max(1, n // target)\nidx = list(range(0, n, step))[:target]\n\nframes = vr.get_batch(idx)  # (T, H, W, C)\nif frames.shape[-1] == 4:\n    frames = frames[..., :3]\nframes = frames.permute(0, 3, 1, 2).contiguous().to(torch.uint8)\nprint(f\\\"Frames: {tuple(frames.shape)} (T,C,H,W)\\\")\n"
    },
    {
      "cell_type": "code",
      "id": "606e4351-2b6b-47d8-8992-89906f3268cb",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n# Build chat and generate (with proper prompt slicing)\nSYSTEM = \"You analyze cataract surgery video frames and answer concisely based only on visible content.\"\nconvo = [\n    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM}]},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"video\", \"video\": video_path},\n        {\"type\": \"text\", \"text\": question},\n    ]},\n]\ntemplated = processor.apply_chat_template(convo, tokenize=False, add_generation_prompt=True)\n\ninputs = processor(text=[templated], videos=[frames], return_tensors=\"pt\", padding=True)\ninputs = {k: (v.to(model.device) if hasattr(v, \"to\") else v) for k, v in inputs.items()}\n\nimport torch\nwith torch.no_grad():\n    output_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False, temperature=0.0)\n\n# Slice off the prompt to get only the new tokens\ngen_only = output_ids[:, inputs[\"input_ids\"].shape[1]:]\nanswer = processor.batch_decode(gen_only, skip_special_tokens=True)[0].strip()\n\nprint(\"\\\\nQ:\", question)\nprint(\"A:\", answer)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}